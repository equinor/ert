on:
  workflow_call:
    inputs:
      os:
        type: string
      python-version:
        type: string
      test-type:
        type: string
      select-string:
        type: string
        default: '""'

env:
  ERT_SHOW_BACKTRACE: 1
  ECL_SKIP_SIGNAL: 1
  ERT_PYTEST_ARGS: '-m ${{ inputs.select-string }} --cov=ert --cov=everest --cov=_ert --cov-report=xml:cov1.xml --junit-xml=junit.xml -o junit_family=legacy -v --durations=25 --benchmark-disable'
  UV_FROZEN: true

jobs:
  tests-ert:
    name: Run ert tests
    runs-on: ${{ inputs.os }}
    timeout-minutes: 30
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        submodules: true
        lfs: true

    - uses: ./.github/actions/install_dependencies_qt
      with:
        os: ${{ inputs.os }}

    - uses: actions/setup-python@v5
      id: setup_python
      with:
        python-version: ${{ inputs.python-version }}

    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
        python-version: ${{ inputs.python-version }}

    - name: Install ert
      run: |
        uv sync --extra everest --extra dev

    - name: GUI Test
      if: inputs.test-type == 'gui-tests'
      run: |
        uv run just ert-gui-tests

    - name: Upload artifact images
      uses: actions/upload-artifact@v4
      if: ${{ failure() }}
      continue-on-error: true
      with:
        name: test-images-${{ github.run_number }}-${{ github.run_id }}
        path: /tmp/tmp*/**/*.png

    - name: CLI Test
      if: inputs.test-type == 'cli-tests'
      run: |
        uv run just ert-cli-tests

    - name: Unit Test
      if: inputs.test-type == 'performance-and-unit-tests'
      run: |
        uv run just ert-unit-tests
        ERT_PYTEST_ARGS='-m ${{ inputs.select-string }} --cov=ert --cov=everest --cov=_ert --cov-report=xml:cov2.xml --junit-xml=junit.xml -o junit_family=legacy -v --durations=25 --benchmark-disable' uv run just ert-doc-tests

    - name: Upload coverage to Codecov
      id: codecov1
      uses: codecov/codecov-action@v5
      continue-on-error: true
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        fail_ci_if_error: true
        files: cov1.xml,cov2.xml
        flags: ${{ inputs.test-type }}
    - name: codecov retry sleep
      if: steps.codecov1.outcome == 'failure'
      run: |
        sleep 30
    - name: Codecov retry
      uses: codecov/codecov-action@v5
      if: steps.codecov1.outcome == 'failure'
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        files: cov1.xml,cov2.xml
        flags: ${{ inputs.test-type }}
        fail_ci_if_error: ${{ github.ref == 'refs/heads/main' }}

    - uses: test-summary/action@v2
      if: ${{ !cancelled() }}
      continue-on-error: true
      with:
        paths: junit.xml

    - name: Upload test results to Codecov
      if: ${{ !cancelled() }}
      uses: codecov/test-results-action@v1
      with:
        token: ${{ secrets.CODECOV_TOKEN }}

    - name: Test CLI
      run: |
        uv run ert --help

    - name: Test for a clean repository
      run: |
        # Remove things we have generated on purpose:
        ls -la
        rm -rf .coverage
        rm -f coverage.xml cov1.xml cov2.xml junit.xml
        rm -f ert.*.whl
        rm -f codecov.SHA*
        rm -f codecov
        git status --porcelain
        test -z "$(git status --porcelain)"
