{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5dddbd",
   "metadata": {},
   "source": [
    "## Random sampling vs. stratified sampling\n",
    "The purpose of this notebook is to build intuition about stratified sampling, by comparing it to random sampling. A guiding question should be\n",
    "\n",
    "> For what ratio of parameters and realizations is the effect of stratified sampling noticeable and advantageous?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfbd953",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import qmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ff56f",
   "metadata": {},
   "source": [
    "### Part I: Sampling a normal distribtuion using random vs. stratified sampling\n",
    "\n",
    "The code snippet below compares two ways to sample from a normal distribution N(0, 1):\n",
    "\n",
    "- Random sampling.\n",
    "- Stratified sampling via Latin Hypercube Sampling (LHS) applied to uniform quantiles in [0, 1], then mapped to N(0, 1) using the inverse CDF (PPF).\n",
    "\n",
    "The code snippet generates a plot that caintains the following:\n",
    "\n",
    "- A histogram of purely random normal samples.\n",
    "- A histogram of stratified samples (quantiles mapped through the normal PPF).\n",
    "- The true normal probability density function.\n",
    "\n",
    "Stratified sampling spreads samples more uniformly across the distribution’s quantiles, often producing a smoother approximation to the true PDF with fewer samples. To see this effect, experiment with the code below by varying the number of samples and the seed. With sufficiently many samples, both methods converge to the same distribution and their histograms will look increasingly similar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9237c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters, please play around with these!\n",
    "num_samples = 200\n",
    "seed = 123\n",
    "\n",
    "\n",
    "# Random normal sample\n",
    "rng = np.random.default_rng(seed)\n",
    "normal_values = rng.normal(loc=0.0, scale=1.0, size=num_samples)\n",
    "\n",
    "# LHS quantiles in [0, 1], mapped to N(0,1) via inverse CDF (PPF)\n",
    "sampler_1d = qmc.LatinHypercube(d=1, seed=seed)\n",
    "quantiles = sampler_1d.random(n=num_samples).ravel()\n",
    "lhs_values = sp.stats.norm.ppf(quantiles)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(normal_values, bins=30, alpha=0.5, density=True, label=\"Random N(0,1)\")\n",
    "ax.hist(\n",
    "    lhs_values,\n",
    "    bins=30,\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    "    label=\"LHS via PPF\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "\n",
    "x = np.linspace(-4, 4, 200)\n",
    "ax.plot(x, sp.stats.norm.pdf(x), \"k--\", label=\"True PDF\")\n",
    "\n",
    "ax.set_title(\"Random vs stratified (LHS) sampling of N(0,1)\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4efe9a",
   "metadata": {},
   "source": [
    "### Part II: Compare the space covrege of stratified sampling vs. random sampling\n",
    "\n",
    "The code snippet below compares how well the two sampling strategies cover a high dimentional cube [0, 1]^d (with d = number_of_parameters) across repeated experiments:\n",
    "\n",
    "- Random sampling: independent uniform draws in [0, 1]^d.\n",
    "- Stratified sampling: Latin Hypercube Sampling (LHS) in [0, 1]^d.\n",
    "\n",
    "For each of the number_of_experiments repetitions, the code draws realizations points for both methods and computes two coverage metrics:\n",
    "\n",
    "- Minimum pairwise distance (“mindist” via qmc.geometric_discrepancy): the smallest Euclidean distance between any two sampled points. Larger values indicate points are more evenly spread (better space-filling), and smaller values indicate a clustering of points. \n",
    "- L2-star discrepancy (“L2-star” via qmc.discrepancy): A way to measure how much the samples deviate from uniform sampling. Smaller values indicate better uniform coverage of the space.\n",
    "\n",
    "The code generates a figure with two histograms:\n",
    "\n",
    "- Min pairwise distance in d dimensions: histograms of min(||x_i − x_j||) across experiments for Random vs. Stratified.\n",
    "- L2-star discrepancy in d dimensions: histograms of the discrepancy metric across experiments for Random vs. Stratified.\n",
    "\n",
    "What to expect:\n",
    "- Modarate number of realizations with few number of parameters: Stratified sampling typically yields larger minimum pairwise distances and lower L2-star discrepancy than purely random sampling, reflecting more uniform, space-filling coverage, and less clustering. \n",
    "- Many parameters with few realizations (n << d): Both methods severely under-sample the space. Stratified sampling helps less in this regime, so the min-distance and discrepancy histograms often look similar and may overlap.\n",
    "- As the number of realizations grows: Both methods converge toward uniform coverage, and their histograms become increasingly similar.\n",
    "\n",
    "To explore the effects:\n",
    "\n",
    "Vary number_of_parameters (dimensionality) and realizations (points per experiment), and adjust number_of_experiments for more stable histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded86dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_comparison(\n",
    "    number_of_parameters: int = 30,\n",
    "    realizations: int = 100,\n",
    "    number_of_experiments: int = 100,\n",
    "    seed: int = 123,\n",
    ") -> None:\n",
    "    \n",
    "    min_dist_random = np.empty(number_of_experiments)\n",
    "    min_dist_lhs = np.empty(number_of_experiments)\n",
    "    l2star_random = np.empty(number_of_experiments)\n",
    "    l2star_lhs = np.empty(number_of_experiments)\n",
    "\n",
    "    for i in range(number_of_experiments):\n",
    "        # Random uniform samples in [0, 1]^d, with d = number of parameters\n",
    "        rng_random = np.random.default_rng(i)\n",
    "        samples_random = rng_random.random(size=(realizations, number_of_parameters))\n",
    "        min_dist_random[i] = qmc.geometric_discrepancy(samples_random, method=\"mindist\")\n",
    "        l2star_random[i] = qmc.discrepancy(samples_random, method=\"L2-star\")\n",
    "\n",
    "        # Latin Hypercube samples in [0, 1]^d, with d = number of parameters\n",
    "        sampler = qmc.LatinHypercube(d=number_of_parameters, rng=rng_random)\n",
    "        samples_lhs = sampler.random(realizations)\n",
    "        min_dist_lhs[i] = qmc.geometric_discrepancy(samples_lhs, method=\"mindist\")\n",
    "        l2star_lhs[i] = qmc.discrepancy(samples_lhs, method=\"L2-star\")\n",
    "\n",
    "    # Plot histograms of the metrics\n",
    "    _fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    binning = \"fd\"  # Freedman–Diaconis rule\n",
    "\n",
    "    axs[0].hist(min_dist_random, bins=binning, alpha=0.5, label=\"Random\")\n",
    "    axs[0].hist(min_dist_lhs, bins=binning, alpha=0.5, label=\"LHS\")\n",
    "    axs[0].set_title(f\"Min pairwise distance in {number_of_parameters}D (higher is better)\")\n",
    "    axs[0].set_xlabel(\"min(||x_i - x_j||)\")\n",
    "    axs[0].set_ylabel(\"Count\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].hist(l2star_random, bins=binning, alpha=0.5, label=\"Random\")\n",
    "    axs[1].hist(l2star_lhs, bins=binning, alpha=0.5, label=\"LHS\")\n",
    "    axs[1].set_title(f\"L2-star discrepancy in {number_of_parameters}D (lower is better)\")\n",
    "    axs[1].set_xlabel(\"Discrepancy\")\n",
    "    axs[1].set_ylabel(\"Count\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_comparison(\n",
    "    number_of_parameters=5, realizations=100, number_of_experiments=1000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ert (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
