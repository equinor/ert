{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83018e13-df59-431c-b550-b0efcb479ee5",
   "metadata": {},
   "source": [
    "# Test new variance inflation algorithm based on KMeans on Drogon data (or other data from ert's storage)\n",
    "\n",
    "1. Run ert on Drogon or on some other case\n",
    "2. Update variable `storage_path` to point to ert's internal storage\n",
    "3. Notebook can now be run to both run the currently implemented autoscaling algorithm and the new one based on KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16daf47d-be4d-4d37-bc8f-2ae2b815b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from ert.storage import open_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd7d3d-b903-4929-b998-96b4ce70a153",
   "metadata": {},
   "source": [
    "# List all available experiments in storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb1f91-4485-4185-910c-01914f396d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_path = \"01_drogon_ahm/storage/\"\n",
    "storage_path = \"/Users/FCUR/git/ert/test-data/ert/poly_example/storage\"\n",
    "with open_storage(storage_path) as storage:\n",
    "    [print(f\"Experiment names: {x.name}\") for x in storage.experiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e99b73-3459-4ba2-9117-50b9eb451ae5",
   "metadata": {},
   "source": [
    "# Pick which experiment to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4ae08-24a4-4632-b1ca-593d52576575",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"ensemble_smoother\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb18f9-e77b-4546-99f6-51418e67a86d",
   "metadata": {},
   "source": [
    "# Load observations and responses from storage. Remove responses with zero standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d333cf-1682-405c-b4d8-49effb4eccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_storage(storage_path, \"r\") as storage:\n",
    "    ensemble = storage.get_experiment_by_name(experiment_name).get_ensemble_by_name(\n",
    "        \"default_0\"\n",
    "    )\n",
    "    selected_obs = ensemble.experiment.observation_keys\n",
    "    iens_active_index = np.flatnonzero(ensemble.get_realization_list_with_responses())\n",
    "    observations_and_responses = ensemble.get_observations_and_responses(\n",
    "        selected_obs, iens_active_index\n",
    "    )\n",
    "\n",
    "response_cols = [str(i) for i in range(1, ensemble.ensemble_size)]\n",
    "df_filtered = observations_and_responses.filter(\n",
    "    pl.concat_list([pl.col(col) for col in response_cols])\n",
    "    .list.eval(pl.element().std())\n",
    "    .list.first()\n",
    "    > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6eb07-d226-46e6-b294-e09ce360a912",
   "metadata": {},
   "source": [
    "# Add column with normalized misfit and sort by it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae1b2e-ee23-4630-a4d1-06884b8b6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the misfit column\n",
    "df_filtered = df_filtered.with_columns(\n",
    "    misfit_normalized=(\n",
    "        pl.concat_list(\n",
    "            [\n",
    "                (pl.col(col) - pl.col(\"observations\")).pow(2) / pl.col(\"std\").pow(2)\n",
    "                for col in response_cols\n",
    "            ]\n",
    "        )\n",
    "        .list.sum()  # Sum the squares\n",
    "        .sqrt()  # Take square root\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get the current column order and insert misfit_normalized after std\n",
    "columns = df_filtered.columns\n",
    "std_index = columns.index(\"std\")\n",
    "new_order = (\n",
    "    columns[: std_index + 1]  # Everything up to and including 'std'\n",
    "    + [\"misfit_normalized\"]  # Insert misfit_normalized\n",
    "    + [\n",
    "        col for col in columns[std_index + 1 :] if col != \"misfit_normalized\"\n",
    "    ]  # Rest of columns\n",
    ")\n",
    "\n",
    "# Reorder and sort\n",
    "df_filtered = df_filtered.select(new_order).sort(\"misfit_normalized\", descending=True)\n",
    "\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20ba23-8265-4e71-a7a7-41d47684e683",
   "metadata": {},
   "source": [
    "# Plot responses with high misfit\n",
    "\n",
    "**Question:** What is a good measure of `coverage`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bc8b2-8bff-43b6-8e75-76f75aeddd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many to plot\n",
    "n_misfits = 5\n",
    "top_misfits = df_filtered.top_k(n_misfits, by=\"misfit_normalized\")\n",
    "\n",
    "# Create subplots for the top 3 misfits\n",
    "fig, axes = plt.subplots(n_misfits, 1, figsize=(12, 15))\n",
    "fig.suptitle(f\"Top {n_misfits} Largest Misfits\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Get data for current row\n",
    "    response_key = top_misfits[\"response_key\"][i]\n",
    "    responses = top_misfits.select(top_misfits.columns[5:-1]).row(\n",
    "        i\n",
    "    )  # Exclude the new misfit_normalized column\n",
    "    observation = top_misfits[\"observations\"][i]\n",
    "    error = top_misfits[\"std\"][i]\n",
    "    misfit_norm = top_misfits[\"misfit_normalized\"][i]\n",
    "\n",
    "    # Plot ensemble responses with better styling\n",
    "    ax.plot(\n",
    "        responses,\n",
    "        \"o-\",\n",
    "        alpha=0.7,\n",
    "        linewidth=1,\n",
    "        markersize=4,\n",
    "        color=\"steelblue\",\n",
    "        label=\"Ensemble responses\",\n",
    "    )\n",
    "\n",
    "    # Add observation and uncertainty bounds with clear styling\n",
    "    ax.axhline(\n",
    "        observation,\n",
    "        color=\"red\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"-\",\n",
    "        label=f\"Observation: {observation:.3f}\",\n",
    "    )\n",
    "    ax.axhline(\n",
    "        observation + error,\n",
    "        color=\"red\",\n",
    "        linewidth=1,\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=\"±1σ bounds\",\n",
    "    )\n",
    "    ax.axhline(observation - error, color=\"red\", linewidth=1, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Fill uncertainty region\n",
    "    ax.fill_between(\n",
    "        range(len(responses)),\n",
    "        observation - error,\n",
    "        observation + error,\n",
    "        alpha=0.2,\n",
    "        color=\"red\",\n",
    "        label=\"Uncertainty band\",\n",
    "    )\n",
    "\n",
    "    # Improve labels and title\n",
    "    ax.set_title(f\"#{i + 1} Misfit: {response_key}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Realization Index\", fontsize=10)\n",
    "    ax.set_ylabel(\"Response Value\", fontsize=10)\n",
    "\n",
    "    # Add grid and legend\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"best\", fontsize=8)\n",
    "\n",
    "    # Add some statistics as text\n",
    "    mean_response = np.mean(responses)\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        f\"Mean ensemble: {mean_response:.3f}\",\n",
    "        transform=ax.transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8),\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bbdbe-362d-4497-863f-ebf3fa99aed6",
   "metadata": {},
   "source": [
    "# Run auto scale exactly as it is implemented in ert\n",
    "\n",
    "TODO: I think I need to include the recent bug-fixes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8db2db-b187-4d7c-afa4-0f9c6a9f8965",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "Y = df_filtered.select(pl.col(response_cols)).to_numpy()\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# First subplot - Unscaled responses\n",
    "ax1.set_title(\"Unscaled responses\")\n",
    "ax1.set_xlabel(\"Realization\")\n",
    "ax1.set_ylabel(\"Response\")\n",
    "for i in range(Y.shape[0]):\n",
    "    ax1.plot(Y[i, :], alpha=0.3)\n",
    "\n",
    "# Calculate scaled responses\n",
    "Y_std = (Y - Y.mean(axis=1, keepdims=True)) / Y.std(axis=1, keepdims=True)\n",
    "\n",
    "# Second subplot - Scaled responses\n",
    "ax2.set_title(\"Scaled responses\")  # Fixed typo: \"set_tiel\" -> \"set_title\"\n",
    "ax2.set_xlabel(\"Realization\")\n",
    "ax2.set_ylabel(\"Response\")\n",
    "for i in range(Y_std.shape[0]):\n",
    "    ax2.plot(Y_std[i, :], alpha=0.3)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "_, s_std, _ = np.linalg.svd(Y_std.T, full_matrices=False)\n",
    "\n",
    "var_ratio_std = np.cumsum(s_std**2) / np.sum(s_std**2)\n",
    "\n",
    "threshold = 0.95\n",
    "# TODO: There's a PR that improves the way we count pca components\n",
    "nr_components_std = max(len([1 for i in var_ratio_std[:-1] if i < threshold]), 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Variance explained\")\n",
    "ax.plot(range(1, len(var_ratio_std) + 1), var_ratio_std, linestyle=\"-\", linewidth=2)\n",
    "ax.axvline(nr_components_std, linestyle=\"--\", alpha=0.7, label=\"95% variance\")\n",
    "ax.annotate(\n",
    "    f\"{nr_components_std} components\\nexplain 95% var\", xy=(nr_components_std + 1, 0.7)\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "correlation = spearmanr(Y_std.T).statistic\n",
    "\n",
    "linkage_matrix = linkage(correlation, \"average\", \"euclidean\")\n",
    "\n",
    "cluster_labels = fcluster(linkage_matrix, nr_components_std, criterion=\"maxclust\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(\n",
    "    cluster_labels, bins=range(1, nr_components_std + 2), alpha=0.7, edgecolor=\"black\"\n",
    ")\n",
    "ax.set_xlabel(\"Cluster Label\")\n",
    "ax.set_ylabel(\"Number of Observations\")\n",
    "ax.set_title(f\"Distribution of Cluster Assignments (k={nr_components_std})\")\n",
    "ax.set_xticks(range(1, nr_components_std + 1))\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482335be-3b49-4fd6-a052-70bb044833f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Alternative method using Kmeans and misfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771833b-7b0f-4a67-9c1e-700d25bb5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters_elbow(data):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters using the \"elbow method\"\n",
    "\n",
    "    The elbow method works like this:\n",
    "    1. Try different numbers of clusters (2, 3, 4, etc.)\n",
    "    2. For each number, measure how \"tight\" the clusters are (inertia)\n",
    "    3. Plot inertia vs number of clusters - it usually looks like an arm\n",
    "    4. The \"elbow\" (bend) in this curve is the optimal number\n",
    "\n",
    "    Think of it like: \"How many groups do I need before adding more groups\n",
    "    doesn't help much?\"\n",
    "    \"\"\"\n",
    "    max_k = min(10, len(data) // 5)  # Don't try more clusters than we have data points\n",
    "    K_range = range(2, max_k + 1)  # Try 2, 3, 4, ... clusters\n",
    "\n",
    "    inertias = []  # This will store how \"spread out\" each clustering solution is\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(data.reshape(-1, 1))\n",
    "        inertias.append(\n",
    "            kmeans.inertia_\n",
    "        )  # inertia = sum of distances from points to cluster centers\n",
    "\n",
    "    # Find the \"elbow\" - where the improvement starts slowing down\n",
    "    if len(inertias) >= 3:\n",
    "        diffs = np.diff(inertias)  # How much inertia decreases each step\n",
    "        diff2 = np.diff(diffs)  # How much the decrease rate changes\n",
    "        elbow_idx = np.argmax(diff2)  # Find biggest change in decrease rate\n",
    "        optimal_k = K_range[elbow_idx + 1]\n",
    "    else:\n",
    "        optimal_k = 3  # If we don't have enough data points, just use 3 clusters\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "\n",
    "def cluster_by_misfit_kmeans_auto(observed, predicted, measurement_std):\n",
    "    \"\"\"\n",
    "    Automatically group responses based on how badly the model fits them.\n",
    "\n",
    "    CORE ASSUMPTION:\n",
    "    ================\n",
    "    Responses with similar misfit patterns are affected by the same underlying\n",
    "    model deficiencies (e.g., wrong permeability zones, missing physics, incorrect\n",
    "    boundary conditions) and should therefore be grouped together for similar\n",
    "    inflation treatment.\n",
    "\n",
    "    The Method:\n",
    "    ===========\n",
    "    Some data points fit our model well (small residuals), others fit poorly\n",
    "    (large residuals). We group responses with similar \"badness of fit\" together,\n",
    "    then apply different scaling factors to each group:\n",
    "    - Low misfit groups: Model works well → trust data highly (low inflation)\n",
    "    - High misfit groups: Systematic model error → reduce trust (high inflation)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    observed : array - what we actually measured in the field\n",
    "    predicted : array - what our model predicted we should measure\n",
    "    measurement_std : array - how accurate our measurements are (measurement error)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_labels : array - which group each data point belongs to\n",
    "    inflation_factors : dict - how much to scale down the importance of each group\n",
    "    n_clusters : int - how many groups we found\n",
    "\n",
    "    Usage in History Matching:\n",
    "    ==========================\n",
    "    Use the inflation_factors to scale measurement error standard deviations:\n",
    "    inflated_std[i] = measurement_std[i] * inflation_factors[cluster_labels[i]]\n",
    "\n",
    "    This reduces the weight of systematically misfitting data during parameter\n",
    "    updates, preventing spurious uncertainty reduction from imperfect models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Calculate how bad each prediction is, accounting for measurement accuracy\n",
    "    #\n",
    "    # Think of this like: \"How many standard deviations off is my prediction?\"\n",
    "    # If observed=100, predicted=90, measurement_std=5:\n",
    "    # standardized_residual = (100-90)/5 = 2 standard deviations off\n",
    "    # We square it to make all values positive and emphasize big errors\n",
    "    raw_residuals = (observed - predicted) / measurement_std  # How many std devs off\n",
    "    standardized_residuals = raw_residuals**2  # Square to get chi-squared\n",
    "\n",
    "    # Step 2: Automatically figure out how many groups we need\n",
    "    # Some responses will fit well (small residuals), others poorly (big residuals)\n",
    "    # The elbow method finds natural groupings in this \"badness of fit\"\n",
    "    n_clusters = find_optimal_clusters_elbow(standardized_residuals)\n",
    "\n",
    "    # Step 3: Group responses by similar \"badness of fit\"\n",
    "    # K-means will find groups where responses within each group have similar\n",
    "    # levels of model error - this implements our core assumption that similar\n",
    "    # misfits indicate similar underlying model deficiencies\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(standardized_residuals.reshape(-1, 1))\n",
    "\n",
    "    # Step 4: Calculate scaling factors for each group based on cluster size\n",
    "    #\n",
    "    # The idea: Responses clustered by similar misfit are affected by the same\n",
    "    # systematic model error and therefore contain redundant information.\n",
    "    # We inflate their variance to prevent this redundant information from\n",
    "    # over-constraining the model parameters.\n",
    "    inflation_factors = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "\n",
    "        # Use square root of cluster size as inflation factor\n",
    "        # This treats clustered responses as having reduced effective degrees of freedom\n",
    "        # - Small clusters (size 2-3): minimal inflation (α ≈ 1.4-1.7)\n",
    "        # - Large clusters (size 16+): significant inflation (α ≥ 4.0)\n",
    "        inflation_factors[cluster_id] = max(1.1, np.sqrt(cluster_size))\n",
    "\n",
    "    return cluster_labels, inflation_factors, n_clusters\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "observations = df_filtered[\"observations\"].to_numpy()\n",
    "std = df_filtered[\"std\"].to_numpy()\n",
    "predicted_values = Y.mean(axis=1)\n",
    "\n",
    "# Run the automatic clustering\n",
    "cluster_labels, inflation_factors, n_clusters = cluster_by_misfit_kmeans_auto(\n",
    "    observed=observations, predicted=predicted_values, measurement_std=std\n",
    ")\n",
    "\n",
    "print(f\"Elbow method selected {n_clusters} clusters\")\n",
    "print(\"\\nInflation factors by cluster:\")\n",
    "print(\"(Higher factors = model fits worse = trust this data less)\")\n",
    "print()\n",
    "\n",
    "for cluster_id, factor in inflation_factors.items():\n",
    "    n_responses = np.sum(cluster_labels == cluster_id)\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "\n",
    "    # Recalculate chi-squared for display\n",
    "    standardized_residuals = ((observations - predicted_values) / std) ** 2\n",
    "    mean_chi2 = np.mean(standardized_residuals[cluster_mask])\n",
    "\n",
    "    print(f\"Cluster {cluster_id}: α={factor:.2f} ({n_responses} responses)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400ed65-cdaf-4e28-9c06-07edd4aef84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
